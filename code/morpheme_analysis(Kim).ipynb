{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#konlpy 라이브러리\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Kkma\n",
    "from eunjeon import Mecab\n",
    "\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#특수문자 제거 정의\n",
    "def clean_signal(text):\n",
    "    cleaned1 = re.sub('[\\｝\\｛\\●\\▣\\①\\◈\\〔\\〈\\「\\〉\\」\\『\\♣\\【\\】\\★\\▶\\》\\《\\☎\\■\\※\\\\n\\\\t\\\\r\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\=\\(\\'\\\"\\♥\\♡\\ㅋ\\ㅠ\\ㅜ\\ㄱ\\ㅎ\\ㄲ\\ㅡ]','',str(text))\n",
    "    return cleaned1\n",
    "\n",
    "def clean_en(text):\n",
    "    cleaned1 = re.sub('[\\』\\］\\［\\〕\\◀\\▼\\♧\\☏\\\\u3000\\…\\｝\\｛\\●\\▣\\①\\◈\\〔\\〈\\「\\〉\\」\\『\\♣\\【\\】\\★\\▶\\》\\《\\☎\\■\\※\\\\n\\\\t\\\\r\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\=\\(\\'\\\"\\♥\\♡\\ㅋ\\ㅠ\\ㅜ\\ㄱ\\ㅎ\\ㄲ\\ㅡ]','',str(text))\n",
    "    cleaned2 = re.sub('[a-zA-Z0-9]','', cleaned1)\n",
    "    return cleaned2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morpheme_hannanum(result_Ser):\n",
    "    han_list = list()\n",
    "\n",
    "    for sentence in range(np.size(result_Ser,0)):\n",
    "        text = result_Ser.values[sentence]\n",
    "        text = clean_signal(text)\n",
    "        text = clean_en(text)\n",
    "        pos = hannanum.pos(text)\n",
    "#         hannanum_word = list()\n",
    "#         hannanum_tag = list()\n",
    "        sub_list = list()\n",
    "        for word, tag in pos:\n",
    "            if tag in ['N']:\n",
    "                sub_list.append(word)\n",
    "            elif tag in ['P']:\n",
    "                if len(word) > 1:\n",
    "                    sub_list.append(word)\n",
    "        han_list.append(sub_list)\n",
    "    return han_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morpheme_okt(result_Ser):\n",
    "    okt_list = list()\n",
    "\n",
    "    for sentence in range(np.size(result_Ser,0)):\n",
    "        text = result_Ser.values[sentence]\n",
    "        text = clean_signal(text)\n",
    "        text = clean_en(text)\n",
    "        pos = okt.pos(text, norm = True)\n",
    "#         okt_word = list()\n",
    "#         okt_tag = list()\n",
    "        sub_list = list()\n",
    "        for word, tag in pos:\n",
    "            if tag in ['Noun']:\n",
    "                sub_list.append(word)\n",
    "            elif tag in ['Verb']:\n",
    "                if len(word) > 1:\n",
    "                    sub_list.append(word)\n",
    "        okt_list.append(sub_list)\n",
    "\n",
    "    return okt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morpheme_kkma(result_Ser):\n",
    "    kkma_list = list()\n",
    "    kkma_plus_list = list()\n",
    "    \n",
    "    for sentence in range(np.size(result_Ser,0)):\n",
    "        text = result_Ser.values[sentence]\n",
    "        text = clean_signal(text)\n",
    "        text = clean_en(text)\n",
    "        pos = kkma.pos(text)\n",
    "    #     kkma_word = list()\n",
    "    #     kkma_tag = list()\n",
    "    \n",
    "        sub_list = list()\n",
    "        sub_plus_list = list()\n",
    "        \n",
    "        for word, tag in pos:\n",
    "            if tag in ['NNG']:\n",
    "                sub_list.append(word)\n",
    "                sub_plus_list.append(word)\n",
    "            elif tag in ['NNP']:\n",
    "                sub_plus_list.append(word)\n",
    "            elif tag in ['NP']:\n",
    "                sub_plus_list.append(word)          \n",
    "            elif tag in ['VV']:\n",
    "                if len(word) > 1:\n",
    "                    sub_list.append(word)\n",
    "                    sub_plus_list.append(word)\n",
    "                    \n",
    "        kkma_plus_list.append(sub_plus_list)\n",
    "        kkma_list.append(sub_list)\n",
    "        \n",
    "\n",
    "    return kkma_list, kkma_plus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morpheme_komoran(result_Ser):\n",
    "    komoran_list = list()\n",
    "    komoran_plus_list = list()\n",
    "    for sentence in range(np.size(result_Ser,0)):\n",
    "        text = result_Ser.values[sentence]\n",
    "        text = clean_signal(text)\n",
    "        text = clean_en(text)\n",
    "        pos = kkma.pos(text)\n",
    "    #     komoran_word = list()\n",
    "    #     komoran_tag = list()\n",
    "    \n",
    "        sub_list = list()\n",
    "        sub_plus_list = list()\n",
    "        \n",
    "        for word, tag in pos:\n",
    "            if tag in ['NNG']:\n",
    "                sub_plus_list.append(word)\n",
    "                sub_list.append(word)\n",
    "            elif tag in ['NNP']:\n",
    "                sub_plus_list.append(word)\n",
    "            elif tag in ['NP']:\n",
    "                sub_plus_list.append(word)\n",
    "            elif tag in ['VV']:\n",
    "                if len(word) > 1:\n",
    "                    sub_list.append(word)\n",
    "                    sub_plus_list.append(word)\n",
    "\n",
    "        komoran_list.append(sub_list)\n",
    "        komoran_plus_list.append(sub_plus_list)\n",
    "    return komoran_list, komoran_plus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morpheme_mecab(result_Ser):\n",
    "    mecab_list = list()\n",
    "    mecab_plus_list = list()\n",
    "\n",
    "    for sentence in range(np.size(result_Ser,0)):\n",
    "        text = result_Ser.values[sentence]\n",
    "        text = clean_signal(text)\n",
    "        text = clean_en(text)\n",
    "        pos = mecab.pos(text)\n",
    "    #     mecab_word = list()\n",
    "    #     mecab_tag = list()\n",
    "    \n",
    "        sub_list = list()\n",
    "        sub_plus_list = list()\n",
    "        \n",
    "        for word, tag in pos:\n",
    "            if tag in ['NNG']:\n",
    "                sub_plus_list.append(word)\n",
    "                sub_list.append(word)\n",
    "            elif tag in ['NNP']:\n",
    "                sub_plus_list.append(word)\n",
    "            elif tag in ['NP']:\n",
    "                sub_plus_list.append(word)\n",
    "            elif tag in ['VV']:\n",
    "                if len(word) > 1:\n",
    "                    sub_plus_list.append(word)\n",
    "                    sub_list.append(word)\n",
    "        mecab_list.append(sub_list)\n",
    "        mecab_plus_list.append(sub_plus_list)\n",
    "    return mecab_list, mecab_plus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_okt(sentences):\n",
    "    byokt = list()\n",
    "    for sentence in sentences:\n",
    "\n",
    "        sub_list = list()\n",
    "        for words in sentence:    \n",
    "            pos = okt.pos(words, norm = True)\n",
    "\n",
    "            for word, tag in pos:\n",
    "                if tag in ['Noun']:\n",
    "                    sub_list.append(word)\n",
    "                elif tag in ['Verb']:\n",
    "                    if len(word)>1:\n",
    "                        sub_list.append(word)\n",
    "        byokt.append(sub_list)\n",
    "    return byokt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(document):\n",
    "    docs = list()\n",
    "    count = 0\n",
    "    for file in document:\n",
    "        count += 1\n",
    "        docs.extend(file)\n",
    "    docs = np.unique(docs)\n",
    "    docs_zero = np.zeros([count, len(docs)])\n",
    "    docs = list(docs)\n",
    "    for i in range(count):\n",
    "        for j in range(len(document[i])):\n",
    "            if document[i][j] in docs:\n",
    "                docs_zero[i][docs.index(document[i][j])] += 1\n",
    "    return docs_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_label(document, label_info):\n",
    "    document = np.append(document, label_info, axis = 1)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hand_data = pd.read_excel('smishing_dataset_SY_.xlsx', sheet_name = 0)\n",
    "police_data = pd.read_excel('smishing_dataset_SY_.xlsx', sheet_name = 1)\n",
    "\n",
    "tr_hand = list()\n",
    "for i in range(1, np.size(hand_data.values,0)):\n",
    "    tr_hand.append(hand_data.values[i][4])\n",
    "    \n",
    "tr_police = list()\n",
    "for i in range(np.size(police_data.values,0)):\n",
    "    tr_police.append(police_data.values[i][3])\n",
    "    \n",
    "tr_data = np.hstack([tr_hand, tr_police])\n",
    "tr_ind = np.ones(np.size(tr_data))\n",
    "tr_Ser = pd.Series(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv('food_2021_12_28_file.txt', sep = '\\n')\n",
    "test_data = pd.read_csv('fashion_2022_1_4_file.txt', sep = '\\n')\n",
    "test_list = list()\n",
    "for i in range(np.size(test_data.values,0)):\n",
    "    test_list.append(test_data.values[i][0])\n",
    "    \n",
    "test_ind = np.zeros(np.size(test_list))\n",
    "\n",
    "test_Ser = pd.Series(test_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_label = np.hstack([tr_ind, test_ind])[:,np.newaxis]\n",
    "result_Ser = pd.concat([tr_Ser, test_Ser], axis = 0)\n",
    "# result_Ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = Hannanum()\n",
    "okt = Okt()\n",
    "komoran = Komoran(max_heap_size= 1024 * 6)\n",
    "kkma = Kkma()\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic morpheme analysis\n",
    "han_sentences = morpheme_hannanum(result_Ser)\n",
    "okt_sentences = morpheme_okt(result_Ser)\n",
    "kkma_sentences, kkma_plus_sentences = morpheme_kkma(result_Ser)\n",
    "komoran_sentences, komoran_plus_sentences = morpheme_komoran(result_Ser)\n",
    "mecab_sentences, mecab_plus_sentences = morpheme_mecab(result_Ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# byokt\n",
    "hannanum_org_byokt = by_okt(han_sentences)\n",
    "okt_org_byokt = by_okt(okt_sentences)\n",
    "kkma_org_byokt = by_okt(kkma_sentences)\n",
    "kkma_plus_byokt = by_okt(kkma_plus_sentences)\n",
    "komoran_org_byokt = by_okt(komoran_sentences)\n",
    "komoran_plus_byokt = by_okt(komoran_plus_sentences)\n",
    "mecab_org_byokt = by_okt(mecab_sentences)\n",
    "mecab_plus_byokt = by_okt(mecab_plus_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creative documentation matrix\n",
    "hannanum_org = create_docs(han_sentences)\n",
    "hannanum_org_byokt = create_docs(hannanum_org_byokt)\n",
    "okt_org = create_docs(okt_sentences)\n",
    "okt_org_byokt = create_docs(okt_org_byokt)\n",
    "kkma_org = create_docs(kkma_sentences)\n",
    "kkma_org_byokt = create_docs(kkma_org_byokt)\n",
    "kkma_plus = create_docs(kkma_plus_sentences)\n",
    "kkma_plus_byokt = create_docs(kkma_plus_byokt)\n",
    "komoran_org = create_docs(komoran_sentences)\n",
    "komoran_org_byokt = create_docs(komoran_org_byokt)\n",
    "komoran_plus = create_docs(komoran_plus_sentences)\n",
    "komoran_plus_byokt = create_docs(komoran_plus_byokt)\n",
    "mecab_org = create_docs(mecab_sentences)\n",
    "mecab_org_byokt = create_docs(mecab_org_byokt)\n",
    "mecab_plus = create_docs(mecab_plus_sentences)\n",
    "mecab_plus_byokt = create_docs(mecab_plus_byokt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add class label\n",
    "\n",
    "hannanum_org = register_label(hannanum_org, result_label)\n",
    "hannanum_org_byokt = register_label(hannanum_org_byokt, result_label)\n",
    "okt_org = register_label(okt_org, result_label)\n",
    "okt_org_byokt = register_label(okt_org_byokt, result_label)\n",
    "kkma_org = register_label(kkma_org, result_label)\n",
    "kkma_org_byokt = register_label(kkma_org_byokt, result_label)\n",
    "kkma_plus = register_label(kkma_plus, result_label)\n",
    "kkma_plus_byokt = register_label(kkma_plus_byokt, result_label)\n",
    "komoran_org = register_label(komoran_org, result_label)\n",
    "komoran_org_byokt = register_label(komoran_org_byokt, result_label)\n",
    "komoran_plus = register_label(komoran_plus, result_label)\n",
    "komoran_plus_byokt = register_label(komoran_plus_byokt, result_label)\n",
    "mecab_org = register_label(mecab_org, result_label)\n",
    "mecab_org_byokt = register_label(mecab_org_byokt, result_label)\n",
    "mecab_plus = register_label(mecab_plus, result_label)\n",
    "mecab_plus_byokt = register_label(mecab_plus_byokt, result_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum_org_df = pd.DataFrame(hannanum_org)\n",
    "hannanum_org_byokt_df = pd.DataFrame(hannanum_org_byokt)\n",
    "okt_org_df = pd.DataFrame(okt_org)\n",
    "okt_org_byokt_df = pd.DataFrame(okt_org_byokt)\n",
    "kkma_org_df = pd.DataFrame(kkma_org)\n",
    "kkma_org_byokt_df = pd.DataFrame(kkma_org_byokt)\n",
    "kkma_plus_df = pd.DataFrame(kkma_plus)\n",
    "kkma_plus_byokt_df = pd.DataFrame(kkma_plus_byokt)\n",
    "komoran_org_df = pd.DataFrame(komoran_org)\n",
    "komoran_org_byokt_df = pd.DataFrame(komoran_org_byokt)\n",
    "komoran_plus_df = pd.DataFrame(komoran_plus)\n",
    "komoran_plus_byokt_df = pd.DataFrame(komoran_plus_byokt)\n",
    "mecab_org_df = pd.DataFrame(mecab_org)\n",
    "mecab_org_byokt_df = pd.DataFrame(mecab_org_byokt)\n",
    "mecab_plus_df = pd.DataFrame(mecab_plus)\n",
    "mecab_plus_byokt_df = pd.DataFrame(mecab_plus_byokt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum_org_df.to_csv(\"hannanum_org_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "hannanum_org_byokt_df.to_csv(\"hannanum_org_byokt_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "okt_org_df.to_csv(\"okt_org_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "okt_org_byokt_df.to_csv(\"okt_org_byokt_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "kkma_org_df.to_csv(\"kkma_org_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "kkma_org_byokt_df.to_csv(\"kkma_org_byokt_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "kkma_plus_df.to_csv(\"kkma_plus_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "kkma_plus_byokt_df.to_csv(\"kkma_plus_byokt_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "komoran_org_df.to_csv(\"komoran_org_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "komoran_org_byokt_df.to_csv(\"komoran_org_byokt_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "komoran_plus_df.to_csv(\"komoran_plus_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "komoran_plus_byokt_df.to_csv(\"komoran_plus_byokt_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "mecab_org_df.to_csv(\"mecab_org_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "mecab_org_byokt_df.to_csv(\"mecab_org_byokt_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "mecab_plus_df.to_csv(\"mecab_plus_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')\n",
    "mecab_plus_byokt_df.to_csv(\"mecab_plus_byokt_SY.csv\", header=True, index=True, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
